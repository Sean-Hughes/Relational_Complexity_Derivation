####################################################################
# dependencies
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(stringr)
library(tokenizers)
library(yarrr)
library(effsize)
?sentiments
View(sentiments)
####################################################################
# Sentiment analysis via tidytext
# Based on tidytext readme: https://github.com/juliasilge/tidytext
# Author: Ian Hussey (ian.hussey@ugent.be)
# Version: 0.1
# exclusions: comments with less than 3 words
########################################################################
# Clean workspace
rm(list=ls())
####################################################################
# dependencies
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(stringr)
library(tokenizers)
library(yarrr)
library(effsize)
####################################################################
# demo using austen data - from tidytext readme: https://github.com/juliasilge/tidytext
# acquire data
reddit_data <-
read.csv("/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
mutate(line_number = row_number()) # add line numbers
# remove non alphanumber or punctuation characters. This was done in tidying, but for some reason if not repeated here unnest_tokens throws an error.
reddit_data$comments <- gsub("[^[:alnum:][:punct:]///' ]", "", reddit_data$comments)
# convert text to tidy text, with one word per row and the line number recorded.
tidy_reddit_data <-
reddit_data %>%
select(line_number, comments) %>%
unnest_tokens(word, comments)  # requires  package
# calculate number of words per comment
words_per_comment <-
tidy_reddit_data %>%
group_by(line_number) %>%
summarise(n = n())
# remove stopwords
data("stop_words")  # from TidyText
stop_words_removed <-
tidy_reddit_data %>%
anti_join(stop_words)
# print most frequent words
stop_words_removed %>%
count(word, sort = TRUE)
# sentiment analysis
NRC <-
sentiments %>%  # from TidyText. Bing Liu's list divides between lists of positive and negative words. "nru" has mutliple emotional categories and may be worth exploring too.
filter(lexicon == "NRC") %>%
select(-score)
comment_sentiment <-
stop_words_removed %>%
inner_join(bing) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment)
rm(list=ls())
####################################################################
# dependencies
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(stringr)
library(tokenizers)
library(yarrr)
library(effsize)
####################################################################
# demo using austen data - from tidytext readme: https://github.com/juliasilge/tidytext
# acquire data
reddit_data <-
read.csv("/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
mutate(line_number = row_number()) # add line numbers
reddit_data <-
read.csv("/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Suicide attempt thread/Data acquisition and cleaning/data/5_data_for_analysis.csv") %>%
mutate(line_number = row_number()) # add line numbers
# remove non alphanumber or punctuation characters. This was done in tidying, but for some reason if not repeated here unnest_tokens throws an error.
reddit_data$comments <- gsub("[^[:alnum:][:punct:]///' ]", "", reddit_data$comments)
tidy_reddit_data <-
reddit_data %>%
select(line_number, comments) %>%
unnest_tokens(word, comments)  # requires  package
# calculate number of words per comment
words_per_comment <-
tidy_reddit_data %>%
group_by(line_number) %>%
summarise(n = n())
# remove stopwords
data("stop_words")  # from TidyText
stop_words_removed <-
tidy_reddit_data %>%
anti_join(stop_words)
# print most frequent words
stop_words_removed %>%
count(word, sort = TRUE)
NRC <-
sentiments %>%  # from TidyText. Bing Liu's list divides between lists of positive and negative words. "nru" has mutliple emotional categories and may be worth exploring too.
filter(lexicon == "NRC") %>%
select(-score)
View(NRC)
View(NRC)
NRC <-
sentiments %>%  # from TidyText. Bing Liu's list divides between lists of positive and negative words. "nru" has mutliple emotional categories and may be worth exploring too.
filter(lexicon == "NRC") %>%
select(-score)
NRC <-
sentiments %>%  # from TidyText. Bing Liu's list divides between lists of positive and negative words. "nru" has mutliple emotional categories and may be worth exploring too.
filter(lexicon == "NRC")
View(NRC)
View(NRC)
NRC <-
sentiments
View(NRC)
View(NRC)
nrc <-
sentiments %>%  # from TidyText. Bing Liu's list divides between lists of positive and negative words. "nru" has mutliple emotional categories and may be worth exploring too.
filter(lexicon == "nrc") %>%
select(-score)
View(nrc)
View(nrc)
comment_sentiment <-
stop_words_removed %>%
inner_join(bing) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment)
comment_sentiment <-
stop_words_removed %>%
inner_join(nrc) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment)
View(comment_sentiment)
View(comment_sentiment)
comment_sentiment <-
stop_words_removed %>%
inner_join(nrc) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment) %>%
mutate(percent_anger = round(anger/n, 3)*100,
percent_anticipation = round(anticipation/n, 3)*100,
percent_disgust = round(disgust/n, 3)*100,
percent_fear = round(fear/n, 3)*100,
percent_joy = round(joy/n, 3)*100,
percent_negative = round(negative/n, 3)*100,
percent_positive = round(positive/n, 3)*100,
percent_sadness = round(sadness/n, 3)*100,
percent_surprise = round(surprise/n, 3)*100,
percent_trust = round(trust/n, 3)*100,
percent_sentiment = round(sentiment/n, 3)*100,
zero = 0) %>%
filter(n >= 1)
View(comment_sentiment)
View(comment_sentiment)
write.csv(comment_sentiment, "/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Quant analysis/sentiment data output - 10 categories.csv", row.names = FALSE)
write.csv(comment_sentiment, "/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Suicide attempt thread/Quant analysis/sentiment data output - 10 categories.csv", row.names = FALSE)
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust,
sentiment = percent_sentiment) %>%
gather(valence,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust,
sentiment))
comment_sentiment <-
stop_words_removed %>%
inner_join(nrc) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment) %>%
mutate(percent_anger = round(anger/n, 3)*100,
percent_anticipation = round(anticipation/n, 3)*100,
percent_disgust = round(disgust/n, 3)*100,
percent_fear = round(fear/n, 3)*100,
percent_joy = round(joy/n, 3)*100,
percent_negative = round(negative/n, 3)*100,
percent_positive = round(positive/n, 3)*100,
percent_sadness = round(sadness/n, 3)*100,
percent_surprise = round(surprise/n, 3)*100,
percent_trust = round(trust/n, 3)*100,
percent_sentiment = round(sentiment/n, 3)*100,
zero = 0) %>%
filter(n >= 1)  # No exclusions based on response length, after inspection of data for meaningfulness.
write.csv(comment_sentiment, "/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Suicide attempt thread/Quant analysis/sentiment data output - 10 categories.csv", row.names = FALSE)
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust,
percent_sentiment) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust,
sentiment = percent_sentiment) %>%
gather(valence,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust,
sentiment))
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust,
percent_sentiment) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust,
sentiment = percent_sentiment) %>%
gather(emotion,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust,
sentiment))
proportions_RDI_graph <-
pirateplot(formula = percent_sentiment_score ~ emotion,
data = percentages_for_graphing,
xlab = "Word valence",
ylab = "Percent",
main = "Comment valences",
theme.o = 1,
pal = "#526273",
ylim = c(0, 100),  ## optionally, trim the data presented
bean.o = 1,
point.o = .2,
bar.o = .5,
hdi.o = 1,
line.o = 0) # invisible line
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust,
percent_sentiment) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust,
sentiment = percent_sentiment) %>%
gather(emotion,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust))
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust) %>%
gather(emotion,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust))
proportions_RDI_graph <-
pirateplot(formula = percent_sentiment_score ~ emotion,
data = percentages_for_graphing,
xlab = "Word valence",
ylab = "Percent",
main = "Comment valences",
theme.o = 1,
pal = "#526273",
ylim = c(0, 100),  ## optionally, trim the data presented
bean.o = 1,
point.o = .2,
bar.o = .5,
hdi.o = 1,
line.o = 0) # invisible line
proportions_RDI_graph <-
pirateplot(formula = percent_sentiment_score ~ emotion,
data = percentages_for_graphing,
xlab = "Word valence",
ylab = "Percent",
main = "Comment valences",
theme.o = 1,
pal = "#526273",
ylim = c(0, 100),  ## optionally, trim the data presented
bean.o = 1,
point.o = .2,
bar.o = .5,
#hdi.o = 1,
line.o = 0) # invisible line
comment_sentiment <-
stop_words_removed %>%
inner_join(nrc) %>%
count(line_number, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
left_join(words_per_comment) %>%
mutate(percent_anger = round(anger/n, 3)*100,
percent_anticipation = round(anticipation/n, 3)*100,
percent_disgust = round(disgust/n, 3)*100,
percent_fear = round(fear/n, 3)*100,
percent_joy = round(joy/n, 3)*100,
percent_negative = round(negative/n, 3)*100,
percent_positive = round(positive/n, 3)*100,
percent_sadness = round(sadness/n, 3)*100,
percent_surprise = round(surprise/n, 3)*100,
percent_trust = round(trust/n, 3)*100,
percent_sentiment = round(sentiment/n, 3)*100,
zero = 0) %>%
filter(n >= 3)  # No exclusions based on response length, after inspection of data for meaningfulness.
percentages_for_graphing <-
select(comment_sentiment,
n,
line_number,
percent_anger,
percent_anticipation,
percent_disgust,
percent_fear,
percent_joy,
percent_negative,
percent_positive,
percent_sadness,
percent_surprise,
percent_trust) %>%
rename(anger = percent_anger,
anticipation = percent_anticipation,
disgust = percent_disgust,
fear = percent_fear,
joy = percent_joy,
negative = percent_negative,
positive = percent_positive,
sadness = percent_sadness,
surprise = percent_surprise,
trust = percent_trust) %>%
gather(emotion,
percent_sentiment_score,
c(anger,
anticipation,
disgust,
fear,
joy,
negative,
positive,
sadness,
surprise,
trust))
proportions_RDI_graph <-
pirateplot(formula = percent_sentiment_score ~ emotion,
data = percentages_for_graphing,
xlab = "Word valence",
ylab = "Percent",
main = "Comment valences",
theme.o = 1,
pal = "#526273",
ylim = c(0, 100),  ## optionally, trim the data presented
bean.o = 1,
point.o = .2,
bar.o = .5,
#hdi.o = 1,
line.o = 0) # invisible line
# save as 5*6
proportions_RDI_graph <-
pirateplot(formula = percent_sentiment_score ~ emotion,
data = percentages_for_graphing,
xlab = "Emotion",
ylab = "Percent",
main = "Emotional words used in comments",
theme.o = 1,
pal = "#526273",
ylim = c(0, 100),  ## optionally, trim the data presented
bean.o = 1,
point.o = .2,
bar.o = .5,
#hdi.o = 1,
line.o = 0) # invisible line
write.csv(comment_sentiment, "/Users/Ian/Dropbox/Work/Studies/Reactions to surviving a suicide attempt - naturalistic data from reddit/Suicide attempt thread/Quant analysis/sentiment data output - 10 categories.csv", row.names = FALSE)
